{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2t6Ked6v3Vmw"
      },
      "source": [
        "#Setting up the Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSzA0pPLMGrd",
        "outputId": "adbab0b9-da5c-422a-b717-da9ba0c48115"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'AI_Agentic_DL'...\n",
            "remote: Enumerating objects: 189, done.\u001b[K\n",
            "remote: Counting objects: 100% (189/189), done.\u001b[K\n",
            "remote: Compressing objects: 100% (140/140), done.\u001b[K\n",
            "Receiving objects: 100% (189/189), 93.41 KiB | 2.46 MiB/s, done.\n",
            "remote: Total 189 (delta 76), reused 138 (delta 36), pack-reused 0 (from 0)\u001b[K\n",
            "Resolving deltas: 100% (76/76), done.\n",
            "/content/AI_Agentic_DL\n",
            "Branch 'model-comparison' set up to track remote branch 'model-comparison' from 'origin'.\n",
            "Switched to a new branch 'model-comparison'\n",
            "Requirement already satisfied: tensorflow>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (2.19.0)\n",
            "Requirement already satisfied: scikit-learn>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (1.6.1)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.24.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (3.10.0)\n",
            "Requirement already satisfied: seaborn>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (0.13.2)\n",
            "Requirement already satisfied: shap>=0.42.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (0.50.0)\n",
            "Requirement already satisfied: kagglehub>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (0.3.13)\n",
            "Requirement already satisfied: transformers>=4.35.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (5.0.0)\n",
            "Requirement already satisfied: accelerate>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (1.12.0)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (2.9.0+cu128)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.15.0->-r requirements.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.15.0->-r requirements.txt (line 1)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.15.0->-r requirements.txt (line 1)) (25.12.19)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.15.0->-r requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.15.0->-r requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.15.0->-r requirements.txt (line 1)) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.15.0->-r requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.15.0->-r requirements.txt (line 1)) (26.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.15.0->-r requirements.txt (line 1)) (5.29.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.15.0->-r requirements.txt (line 1)) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.15.0->-r requirements.txt (line 1)) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.15.0->-r requirements.txt (line 1)) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.15.0->-r requirements.txt (line 1)) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.15.0->-r requirements.txt (line 1)) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.15.0->-r requirements.txt (line 1)) (2.1.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.15.0->-r requirements.txt (line 1)) (1.78.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.15.0->-r requirements.txt (line 1)) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.15.0->-r requirements.txt (line 1)) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.15.0->-r requirements.txt (line 1)) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.15.0->-r requirements.txt (line 1)) (0.5.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.3.0->-r requirements.txt (line 2)) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.3.0->-r requirements.txt (line 2)) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.3.0->-r requirements.txt (line 2)) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 3)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 3)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 3)) (2025.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (3.3.2)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.12/dist-packages (from shap>=0.42.0->-r requirements.txt (line 7)) (4.67.3)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.12/dist-packages (from shap>=0.42.0->-r requirements.txt (line 7)) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.12/dist-packages (from shap>=0.42.0->-r requirements.txt (line 7)) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from shap>=0.42.0->-r requirements.txt (line 7)) (3.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub>=0.1.0->-r requirements.txt (line 8)) (6.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers>=4.35.0->-r requirements.txt (line 9)) (3.21.0)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.35.0->-r requirements.txt (line 9)) (1.4.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.35.0->-r requirements.txt (line 9)) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.35.0->-r requirements.txt (line 9)) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers>=4.35.0->-r requirements.txt (line 9)) (0.23.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.35.0->-r requirements.txt (line 9)) (0.7.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.24.0->-r requirements.txt (line 10)) (5.9.5)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (3.5.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow>=2.15.0->-r requirements.txt (line 1)) (0.46.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers>=4.35.0->-r requirements.txt (line 9)) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers>=4.35.0->-r requirements.txt (line 9)) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers>=4.35.0->-r requirements.txt (line 9)) (1.5.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=2.15.0->-r requirements.txt (line 1)) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=2.15.0->-r requirements.txt (line 1)) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=2.15.0->-r requirements.txt (line 1)) (0.18.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.54->shap>=0.42.0->-r requirements.txt (line 7)) (0.43.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.15.0->-r requirements.txt (line 1)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.15.0->-r requirements.txt (line 1)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.15.0->-r requirements.txt (line 1)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.15.0->-r requirements.txt (line 1)) (2026.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.1.0->-r requirements.txt (line 11)) (1.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=2.15.0->-r requirements.txt (line 1)) (3.10.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=2.15.0->-r requirements.txt (line 1)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=2.15.0->-r requirements.txt (line 1)) (3.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.1.0->-r requirements.txt (line 11)) (3.0.3)\n",
            "Requirement already satisfied: typer>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers>=4.35.0->-r requirements.txt (line 9)) (0.23.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers>=4.35.0->-r requirements.txt (line 9)) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers>=4.35.0->-r requirements.txt (line 9)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers>=4.35.0->-r requirements.txt (line 9)) (0.16.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.23.0->typer-slim->transformers>=4.35.0->-r requirements.txt (line 9)) (8.3.1)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from typer>=0.23.0->typer-slim->transformers>=4.35.0->-r requirements.txt (line 9)) (0.0.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow>=2.15.0->-r requirements.txt (line 1)) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow>=2.15.0->-r requirements.txt (line 1)) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow>=2.15.0->-r requirements.txt (line 1)) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/lawapaul/AI_Agentic_DL.git\n",
        "%cd AI_Agentic_DL\n",
        "!git checkout model-comparison\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvTmemKE3Twh"
      },
      "source": [
        "#Downloading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "BayaSqrcOJvn",
        "outputId": "98eac761-ea7a-4394-93d1-17d05a4b535d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-446f6449-dbd7-440b-a30f-c08d31fee3f5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-446f6449-dbd7-440b-a30f-c08d31fee3f5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"hss501\",\"key\":\"05c56fa04dc4e6c3879ca3f6e0ce351c\"}'}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TCM3HOOf2acm"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wph_NLUY23Fy",
        "outputId": "8b7dd793-5d92-4418-b734-c861698c20d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/chethuhn/network-intrusion-dataset\n",
            "License(s): CC0-1.0\n",
            "Downloading network-intrusion-dataset.zip to /content/AI_Agentic_DL\n",
            " 64% 148M/230M [00:00<00:00, 1.55GB/s]\n",
            "100% 230M/230M [00:00<00:00, 884MB/s] \n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d chethuhn/network-intrusion-dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6K2UkDy_3LgA",
        "outputId": "ef64e7bf-b091-43ae-cea5-8991c19b3aa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  network-intrusion-dataset.zip\n",
            "  inflating: cicids2017/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv  \n",
            "  inflating: cicids2017/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv  \n",
            "  inflating: cicids2017/Friday-WorkingHours-Morning.pcap_ISCX.csv  \n",
            "  inflating: cicids2017/Monday-WorkingHours.pcap_ISCX.csv  \n",
            "  inflating: cicids2017/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv  \n",
            "  inflating: cicids2017/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv  \n",
            "  inflating: cicids2017/Tuesday-WorkingHours.pcap_ISCX.csv  \n",
            "  inflating: cicids2017/Wednesday-workingHours.pcap_ISCX.csv  \n"
          ]
        }
      ],
      "source": [
        "!unzip -o network-intrusion-dataset.zip -d cicids2017\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LF8Hgwk93Nze",
        "outputId": "44709c46-e2bf-42bd-88f6-99c6a7871ca3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
            "Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
            "Friday-WorkingHours-Morning.pcap_ISCX.csv\n",
            "Monday-WorkingHours.pcap_ISCX.csv\n",
            "Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\n",
            "Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
            "Tuesday-WorkingHours.pcap_ISCX.csv\n",
            "Wednesday-workingHours.pcap_ISCX.csv\n"
          ]
        }
      ],
      "source": [
        "!ls cicids2017\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQnd16ud3Qr-"
      },
      "source": [
        "#Merging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7p7jl874Ubj",
        "outputId": "89b323d5-3052-4830-ef43-53746071dc2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading: cicids2017/Tuesday-WorkingHours.pcap_ISCX.csv\n",
            "Loading: cicids2017/Monday-WorkingHours.pcap_ISCX.csv\n",
            "Loading: cicids2017/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
            "Loading: cicids2017/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\n",
            "Loading: cicids2017/Wednesday-workingHours.pcap_ISCX.csv\n",
            "Loading: cicids2017/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
            "Loading: cicids2017/Friday-WorkingHours-Morning.pcap_ISCX.csv\n",
            "Loading: cicids2017/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
            "Final Shape: (2830743, 79)\n",
            "Column names sample: Index(['Destination Port', 'Flow Duration', 'Total Fwd Packets',\n",
            "       'Total Backward Packets', 'Total Length of Fwd Packets'],\n",
            "      dtype='object')\n",
            "Total Classes: 15\n",
            "\n",
            "Class Distribution:\n",
            "\n",
            "Label\n",
            "BENIGN                        2273097\n",
            "DoS Hulk                       231073\n",
            "PortScan                       158930\n",
            "DDoS                           128027\n",
            "DoS GoldenEye                   10293\n",
            "FTP-Patator                      7938\n",
            "SSH-Patator                      5897\n",
            "DoS slowloris                    5796\n",
            "DoS Slowhttptest                 5499\n",
            "Bot                              1966\n",
            "Web Attack � Brute Force         1507\n",
            "Web Attack � XSS                  652\n",
            "Infiltration                       36\n",
            "Web Attack � Sql Injection         21\n",
            "Heartbleed                         11\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "path = \"cicids2017/*.csv\"\n",
        "files = glob.glob(path)\n",
        "\n",
        "df_list = []\n",
        "\n",
        "for file in files:\n",
        "    print(\"Loading:\", file)\n",
        "    temp = pd.read_csv(file)\n",
        "    temp.columns = temp.columns.str.strip()\n",
        "\n",
        "    df_list.append(temp)\n",
        "\n",
        "df = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "print(\"Final Shape:\", df.shape)\n",
        "print(\"Column names sample:\", df.columns[:5])\n",
        "print(\"Total Classes:\", df['Label'].nunique())\n",
        "print(\"\\nClass Distribution:\\n\")\n",
        "print(df['Label'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsRGpcUu3jpH"
      },
      "source": [
        "#Model Comparison File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAgLtLOC4GrK",
        "outputId": "afd8da7e-1651-4db7-aced-437c18546df0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects:  14% (1/7)\u001b[K\rremote: Counting objects:  28% (2/7)\u001b[K\rremote: Counting objects:  42% (3/7)\u001b[K\rremote: Counting objects:  57% (4/7)\u001b[K\rremote: Counting objects:  71% (5/7)\u001b[K\rremote: Counting objects:  85% (6/7)\u001b[K\rremote: Counting objects: 100% (7/7)\u001b[K\rremote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1)\u001b[K\rremote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 4 (delta 3), reused 4 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects:  25% (1/4)\rUnpacking objects:  50% (2/4)\rUnpacking objects:  75% (3/4)\rUnpacking objects: 100% (4/4)\rUnpacking objects: 100% (4/4), 605 bytes | 201.00 KiB/s, done.\n",
            "From https://github.com/lawapaul/AI_Agentic_DL\n",
            " * branch            model-comparison -> FETCH_HEAD\n",
            "   c33d0b5..21e16ef  model-comparison -> origin/model-comparison\n",
            "Updating c33d0b5..21e16ef\n",
            "Fast-forward\n",
            " experiments/top_models_full_training.py | 17 \u001b[32m++++++++++++++\u001b[m\u001b[31m---\u001b[m\n",
            " 1 file changed, 14 insertions(+), 3 deletions(-)\n"
          ]
        }
      ],
      "source": [
        "!git pull origin model-comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOmihv3K4EC7"
      },
      "source": [
        "#Running Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlIj_uRsN66i",
        "outputId": "78d93dae-18da-4fe0-a23e-f61576ba1204"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== FULL DATASET TRAINING (NO SAMPLING) ===\n",
            "\n",
            "\n",
            "=== Starting Preprocessing ===\n",
            "Initial samples: 2830743\n",
            "Initial features: 78\n",
            "Unique classes: 15\n",
            "After cleaning: (2827876, 78)\n",
            "Label mapping done.\n",
            "Scaling complete.\n",
            "\n",
            "Splitting dataset...\n",
            "Train shape: (2262300, 78)\n",
            "Test shape: (565576, 78)\n",
            "Train size: (2262300, 78, 1)\n",
            "Test size: (565576, 78, 1)\n",
            "\n",
            "==============================\n",
            "Training HYBRID on FULL DATASET\n",
            "==============================\n",
            "\n",
            "=== Training IDS HYBRID Model ===\n",
            "Training samples: 2262300\n",
            "Validation samples: 565576\n",
            "Batch size: 128\n",
            "Max epochs: 20\n",
            "Input shape after reshape: (2262300, 78, 1)\n",
            "Epoch 1/20\n",
            "\u001b[1m17675/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9696 - loss: 0.0831\n",
            "Epoch 1: val_accuracy improved from -inf to 0.98511, saving model to saved_models/ids_model.keras\n",
            "\u001b[1m17675/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 10ms/step - accuracy: 0.9696 - loss: 0.0831 - val_accuracy: 0.9851 - val_loss: 0.0405 - learning_rate: 0.0010\n",
            "Epoch 2/20\n",
            "\u001b[1m17675/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9836 - loss: 0.0401\n",
            "Epoch 2: val_accuracy did not improve from 0.98511\n",
            "\u001b[1m17675/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 9ms/step - accuracy: 0.9836 - loss: 0.0401 - val_accuracy: 0.9582 - val_loss: 0.1974 - learning_rate: 0.0010\n",
            "Epoch 3/20\n",
            "\u001b[1m17669/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9848 - loss: 0.0373\n",
            "Epoch 3: val_accuracy did not improve from 0.98511\n",
            "\u001b[1m17675/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 9ms/step - accuracy: 0.9848 - loss: 0.0373 - val_accuracy: 0.9836 - val_loss: 0.0353 - learning_rate: 0.0010\n",
            "Epoch 4/20\n",
            "\u001b[1m17673/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9860 - loss: 0.0342\n",
            "Epoch 4: val_accuracy improved from 0.98511 to 0.98675, saving model to saved_models/ids_model.keras\n",
            "\u001b[1m17675/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 9ms/step - accuracy: 0.9860 - loss: 0.0342 - val_accuracy: 0.9867 - val_loss: 0.0322 - learning_rate: 0.0010\n",
            "Epoch 5/20\n",
            "\u001b[1m17674/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9863 - loss: 0.0333\n",
            "Epoch 5: val_accuracy did not improve from 0.98675\n",
            "\u001b[1m17675/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 9ms/step - accuracy: 0.9863 - loss: 0.0333 - val_accuracy: 0.9393 - val_loss: 0.1988 - learning_rate: 0.0010\n",
            "Epoch 6/20\n",
            "\u001b[1m17675/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9865 - loss: 0.0328\n",
            "Epoch 6: val_accuracy did not improve from 0.98675\n",
            "\u001b[1m17675/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 10ms/step - accuracy: 0.9865 - loss: 0.0328 - val_accuracy: 0.9734 - val_loss: 0.0742 - learning_rate: 0.0010\n",
            "Epoch 7/20\n",
            "\u001b[1m17670/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9871 - loss: 0.0314\n",
            "Epoch 7: val_accuracy did not improve from 0.98675\n",
            "\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m17675/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 10ms/step - accuracy: 0.9871 - loss: 0.0314 - val_accuracy: 0.8779 - val_loss: 0.3794 - learning_rate: 0.0010\n",
            "Epoch 8/20\n",
            "\u001b[1m17673/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9946 - loss: 0.0168\n",
            "Epoch 8: val_accuracy did not improve from 0.98675\n",
            "\u001b[1m17675/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 9ms/step - accuracy: 0.9946 - loss: 0.0168 - val_accuracy: 0.9580 - val_loss: 0.3768 - learning_rate: 5.0000e-04\n",
            "Epoch 9/20\n",
            "\u001b[1m17671/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9958 - loss: 0.0134\n",
            "Epoch 9: val_accuracy improved from 0.98675 to 0.99618, saving model to saved_models/ids_model.keras\n",
            "\u001b[1m17675/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 9ms/step - accuracy: 0.9958 - loss: 0.0134 - val_accuracy: 0.9962 - val_loss: 0.0132 - learning_rate: 5.0000e-04\n",
            "Epoch 10/20\n",
            "\u001b[1m17671/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9952 - loss: 0.0143\n",
            "Epoch 10: val_accuracy did not improve from 0.99618\n",
            "\u001b[1m17675/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 9ms/step - accuracy: 0.9952 - loss: 0.0143 - val_accuracy: 0.9961 - val_loss: 0.0139 - learning_rate: 5.0000e-04\n",
            "Epoch 11/20\n",
            "\u001b[1m17672/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9961 - loss: 0.0123\n",
            "Epoch 11: val_accuracy improved from 0.99618 to 0.99662, saving model to saved_models/ids_model.keras\n",
            "\u001b[1m17675/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 9ms/step - accuracy: 0.9961 - loss: 0.0123 - val_accuracy: 0.9966 - val_loss: 0.0117 - learning_rate: 5.0000e-04\n",
            "Epoch 12/20\n",
            "\u001b[1m17669/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9958 - loss: 0.0127\n",
            "Epoch 12: val_accuracy did not improve from 0.99662\n",
            "\u001b[1m17675/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 10ms/step - accuracy: 0.9958 - loss: 0.0127 - val_accuracy: 0.9958 - val_loss: 0.0138 - learning_rate: 5.0000e-04\n",
            "Epoch 13/20\n",
            "\u001b[1m17674/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9962 - loss: 0.0118\n",
            "Epoch 13: val_accuracy did not improve from 0.99662\n",
            "\u001b[1m17675/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 9ms/step - accuracy: 0.9962 - loss: 0.0118 - val_accuracy: 0.5233 - val_loss: 1.3824 - learning_rate: 5.0000e-04\n",
            "Epoch 14/20\n",
            "\u001b[1m17671/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9965 - loss: 0.0111\n",
            "Epoch 14: val_accuracy improved from 0.99662 to 0.99701, saving model to saved_models/ids_model.keras\n",
            "\u001b[1m17675/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 9ms/step - accuracy: 0.9965 - loss: 0.0111 - val_accuracy: 0.9970 - val_loss: 0.0100 - learning_rate: 5.0000e-04\n",
            "Epoch 15/20\n",
            "\u001b[1m17673/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9964 - loss: 0.0114\n",
            "Epoch 15: val_accuracy did not improve from 0.99701\n",
            "\u001b[1m17675/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 9ms/step - accuracy: 0.9964 - loss: 0.0114 - val_accuracy: 0.8573 - val_loss: 0.4418 - learning_rate: 5.0000e-04\n",
            "Epoch 16/20\n",
            "\u001b[1m17672/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9966 - loss: 0.0106\n",
            "Epoch 16: val_accuracy did not improve from 0.99701\n",
            "\u001b[1m17675/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 10ms/step - accuracy: 0.9966 - loss: 0.0106 - val_accuracy: 0.9961 - val_loss: 0.0130 - learning_rate: 5.0000e-04\n",
            "Epoch 17/20\n",
            "\u001b[1m17669/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9966 - loss: 0.0108\n",
            "Epoch 17: val_accuracy did not improve from 0.99701\n",
            "\n",
            "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m17675/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 9ms/step - accuracy: 0.9966 - loss: 0.0108 - val_accuracy: 0.9950 - val_loss: 0.0158 - learning_rate: 5.0000e-04\n",
            "Epoch 18/20\n",
            "\u001b[1m17674/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9969 - loss: 0.0092\n",
            "Epoch 18: val_accuracy did not improve from 0.99701\n",
            "\u001b[1m17675/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 10ms/step - accuracy: 0.9969 - loss: 0.0092 - val_accuracy: 0.9970 - val_loss: 0.0096 - learning_rate: 2.5000e-04\n",
            "Epoch 19/20\n",
            "\u001b[1m17673/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9971 - loss: 0.0087\n",
            "Epoch 19: val_accuracy improved from 0.99701 to 0.99709, saving model to saved_models/ids_model.keras\n",
            "\u001b[1m17675/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 10ms/step - accuracy: 0.9971 - loss: 0.0087 - val_accuracy: 0.9971 - val_loss: 0.0087 - learning_rate: 2.5000e-04\n",
            "Epoch 20/20\n",
            "\u001b[1m17669/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9972 - loss: 0.0086\n",
            "Epoch 20: val_accuracy improved from 0.99709 to 0.99744, saving model to saved_models/ids_model.keras\n",
            "\u001b[1m17675/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 10ms/step - accuracy: 0.9972 - loss: 0.0086 - val_accuracy: 0.9974 - val_loss: 0.0090 - learning_rate: 2.5000e-04\n",
            "Restoring model weights from the end of the best epoch: 19.\n",
            "\n",
            "=== Training Complete ===\n",
            "\n",
            "=== Evaluating Model ===\n",
            "Test Loss: 0.0087\n",
            "Test Accuracy: 0.9971\n",
            "\n",
            "=== Evaluating Model ===\n",
            "Test Loss: 0.0087\n",
            "Test Accuracy: 0.9971\n",
            "\n",
            "==============================\n",
            "Training RESNET on FULL DATASET\n",
            "==============================\n",
            "\n",
            "=== Training IDS RESNET Model ===\n",
            "Training samples: 2262300\n",
            "Validation samples: 565576\n",
            "Batch size: 128\n",
            "Max epochs: 20\n",
            "Input shape after reshape: (2262300, 78, 1)\n",
            "Epoch 1/20\n",
            "\u001b[1m17675/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9714 - loss: 0.0830\n",
            "Epoch 1: val_accuracy improved from -inf to 0.95158, saving model to saved_models/ids_model.keras\n",
            "\u001b[1m17675/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m272s\u001b[0m 14ms/step - accuracy: 0.9714 - loss: 0.0830 - val_accuracy: 0.9516 - val_loss: 0.2526 - learning_rate: 0.0010\n",
            "Epoch 2/20\n",
            "\u001b[1m17672/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9853 - loss: 0.0374\n",
            "Epoch 2: val_accuracy did not improve from 0.95158\n",
            "\u001b[1m17675/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 14ms/step - accuracy: 0.9853 - loss: 0.0374 - val_accuracy: 0.9410 - val_loss: 0.9152 - learning_rate: 0.0010\n",
            "Epoch 3/20\n",
            "\u001b[1m17675/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9865 - loss: 0.0343\n",
            "Epoch 3: val_accuracy did not improve from 0.95158\n",
            "\u001b[1m17675/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 14ms/step - accuracy: 0.9865 - loss: 0.0343 - val_accuracy: 0.9364 - val_loss: 0.8776 - learning_rate: 0.0010\n",
            "Epoch 4/20\n",
            "\u001b[1m17671/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9870 - loss: 0.0328\n",
            "Epoch 4: val_accuracy did not improve from 0.95158\n",
            "\n",
            "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m17675/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 14ms/step - accuracy: 0.9870 - loss: 0.0328 - val_accuracy: 0.9473 - val_loss: 0.7241 - learning_rate: 0.0010\n",
            "Epoch 5/20\n",
            "\u001b[1m17671/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9900 - loss: 0.0255\n",
            "Epoch 5: val_accuracy did not improve from 0.95158\n",
            "\u001b[1m17675/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 14ms/step - accuracy: 0.9900 - loss: 0.0255 - val_accuracy: 0.9433 - val_loss: 1.0294 - learning_rate: 5.0000e-04\n",
            "Epoch 6/20\n",
            "\u001b[1m17674/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9962 - loss: 0.0129\n",
            "Epoch 6: val_accuracy did not improve from 0.95158\n",
            "\u001b[1m17675/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m241s\u001b[0m 14ms/step - accuracy: 0.9962 - loss: 0.0129 - val_accuracy: 0.9496 - val_loss: 0.7833 - learning_rate: 5.0000e-04\n",
            "Epoch 6: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\n",
            "=== Training Complete ===\n",
            "\n",
            "=== Evaluating Model ===\n",
            "Test Loss: 0.2526\n",
            "Test Accuracy: 0.9516\n",
            "\n",
            "=== Evaluating Model ===\n",
            "Test Loss: 0.2526\n",
            "Test Accuracy: 0.9516\n",
            "\n",
            "Results saved successfully.\n"
          ]
        }
      ],
      "source": [
        "from experiments.top_models_full_training import run_full_training\n",
        "\n",
        "run_full_training(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3rtKQ6V__A0"
      },
      "source": [
        "## 🔎 Conclusion: Full Dataset Training on CICIDS2017\n",
        "\n",
        "In this experiment, two deep learning architectures — **Hybrid** and **ResNet** — were trained on the complete CICIDS2017 dataset (2.83 million samples, 15 classes) without any sampling.\n",
        "\n",
        "### 📊 Performance Summary\n",
        "\n",
        "* **Hybrid Model**\n",
        "\n",
        "  * Test Accuracy: **99.71%**\n",
        "  * Test Loss: **0.0087**\n",
        "  * Stable convergence after learning rate reduction\n",
        "  * Best validation accuracy: **99.74%**\n",
        "\n",
        "* **ResNet Model**\n",
        "\n",
        "  * Test Accuracy: **95.16%**\n",
        "  * Test Loss: **0.2526**\n",
        "  * Early stopping triggered at Epoch 6\n",
        "  * Validation performance remained unstable\n",
        "\n",
        "---\n",
        "\n",
        "## 📈 Model Generalization & Overfitting Analysis\n",
        "\n",
        "### Hybrid Model\n",
        "\n",
        "The Hybrid model shows:\n",
        "\n",
        "* Training accuracy ≈ 99.7%\n",
        "* Validation accuracy ≈ 99.74%\n",
        "* Test accuracy ≈ 99.71%\n",
        "\n",
        "Since training, validation, and test accuracies are very close, **there is no significant overfitting**. The model generalizes well across unseen data.\n",
        "\n",
        "Temporary drops in validation accuracy during training are likely due to:\n",
        "\n",
        "* Extreme class imbalance (e.g., Heartbleed: 11 samples)\n",
        "* Rare-class sensitivity in multi-class classification\n",
        "* Learning rate adjustments during optimization\n",
        "\n",
        "However, the model recovered and achieved stable high performance.\n",
        "\n",
        "### ResNet Model\n",
        "\n",
        "The ResNet architecture exhibited:\n",
        "\n",
        "* High training accuracy\n",
        "* Lower and unstable validation accuracy\n",
        "* Early stopping due to lack of improvement\n",
        "\n",
        "This indicates **mild overfitting and poor generalization** compared to the Hybrid model. ResNet appears less suitable for structured tabular intrusion detection data.\n",
        "\n",
        "---\n",
        "\n",
        "## 🏆 Final Conclusion\n",
        "\n",
        "The **Hybrid architecture clearly outperforms ResNet** for full-scale CICIDS2017 intrusion detection. It achieves near state-of-the-art accuracy while maintaining strong generalization.\n",
        "\n",
        "For structured tabular cybersecurity datasets:\n",
        "\n",
        "* Hybrid / MLP-based architectures are more effective\n",
        "* Deep CNN-style ResNet architectures may not provide additional benefit\n",
        "\n",
        "The experiment confirms that the Hybrid model is the optimal choice for large-scale intrusion detection on CICIDS2017.\n",
        "\n",
        "---\n",
        "\n",
        "✅ Full dataset training completed successfully\n",
        "✅ No significant overfitting in Hybrid model\n",
        "✅ Results saved for further analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GR1Ngtt0hGX7",
        "outputId": "c4cd7bff-979e-4b5e-bb38-a9064c5dc9f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_comparison_results.csv  top_models_full_20epoch.csv\n"
          ]
        }
      ],
      "source": [
        "!ls experiments/results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dUrf52ghlJv"
      },
      "outputs": [],
      "source": [
        "!git config --global user.email \"harshitshekhawat501@gmail.com\"\n",
        "!git config --global user.name \"lawapaul\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLREomXMh8_U",
        "outputId": "78374eb6-7243-4b4b-f175-9b203ba49b03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[model-comparison 604da2d] Full dataset (2.8M) training results - Hybrid vs ResNet (20 epochs)\n",
            " 1 file changed, 3 insertions(+)\n",
            " create mode 100644 experiments/results/top_models_full_20epoch.csv\n"
          ]
        }
      ],
      "source": [
        "!git add experiments/results/top_models_full_20epoch.csv\n",
        "!git commit -m \"Full dataset (2.8M) training results - Hybrid vs ResNet (20 epochs)\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-SPcsXBhIw2",
        "outputId": "dfb91ab1-5c75-4c0f-cbbd-cfeacee58f01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enumerating objects: 8, done.\n",
            "Counting objects:  12% (1/8)\rCounting objects:  25% (2/8)\rCounting objects:  37% (3/8)\rCounting objects:  50% (4/8)\rCounting objects:  62% (5/8)\rCounting objects:  75% (6/8)\rCounting objects:  87% (7/8)\rCounting objects: 100% (8/8)\rCounting objects: 100% (8/8), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects:  20% (1/5)\rCompressing objects:  40% (2/5)\rCompressing objects:  60% (3/5)\rCompressing objects:  80% (4/5)\rCompressing objects: 100% (5/5)\rCompressing objects: 100% (5/5), done.\n",
            "Writing objects:  20% (1/5)\rWriting objects:  40% (2/5)\rWriting objects:  60% (3/5)\rWriting objects:  80% (4/5)\rWriting objects: 100% (5/5)\rWriting objects: 100% (5/5), 622 bytes | 622.00 KiB/s, done.\n",
            "Total 5 (delta 2), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
            "remote: This repository moved. Please use the new location:\u001b[K\n",
            "remote:   https://github.com/Lawapaul/AI_Agentic_DL.git\u001b[K\n",
            "To https://github.com/lawapaul/AI_Agentic_DL.git\n",
            "   21e16ef..604da2d  model-comparison -> model-comparison\n"
          ]
        }
      ],
      "source": [
        "!git push origin model-comparison"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
